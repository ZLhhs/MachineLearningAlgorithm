NaiveBayes2_ReadMe.doc
LiangZHANG
2015-8-6
Liangzxdu@foxmail.com

关键词：朴素贝叶斯算法，文档分类

这是NaiveBayes2.py代码文件的设计手册，该代码文件（NaiveBayes2.py）和本设计手册（NaiveBayes2_ReadMe.doc）同样位于NaiveBayes文件夹中。

NaiveBayes2.py的数据（邮件数据）来自于data2文件夹（同样位于NaiveBayes Project文件夹中），数据如下所示：

第一行的0/1表示该文本是否为垃圾邮件(0为垃圾邮件，1为正常邮件)
从第二行到最后都是邮件正文

同时为了防止统计数据不足导致的概率值为零的情况，我们采用Laplace Smooth来避免（一开始没用，正确率是90%）。同时，使用exp/log来进行概率值映射，防止连乘导致的下溢为零。

程序的基本思路：
1.扫描全文当，构建词典（单独写一个过滤函数，后面也会用到）
2.构建参数矩阵
3.构建概率矩阵（Laplace_Smooth and exp/log）
4.基于测试集进行测试（Prediction）

主要函数剖析：
1.Scan_Text() :
　　主要工作：对整个文本集合中的所有文本（来自“data2文件夹”）进行扫描，预处理（过			滤函数Filter）后，按空格进行切分（split），得到词表/词典，最终得到一个排			好序的list
　　输入：	NULL（或者也可以是路径地址）
　　输出：	一个排好序的list词典L，汇集了所有文本中的全部单词
　　
2.Filter() :
　　主要工作：对于输入的字符串，过滤掉其中的数字，标点符号，并将大写字母转换成小写			形式，便于后期的处理（由于python中字符串是不可变的，所以需要将字符串			先转换为list，处理完后再从list转换回字符串，最终返回该字符串）
　　输入：	待过滤的字符串str
　　输出：	过滤之后的字符串str’

3.Create_Matrix():
　　主要工作：对训练数据集（data2/TrainSet）进行扫描，对每一个文档，读取后，用Filter				过滤，得到该文档包含的单词的list。随后对list中的每个单词，每出现一次，			便在计数矩阵中将其计数加一。有两个计数矩阵，分别是y计数矩阵和word-y			计数矩阵(size : 1*2 and len(L)*2)，最后返回这两个计数矩阵。
　　输入：	词典L
　　输出：	两个计数矩阵(Matrix_Y and Matrix_WordY)，分别是1*2和len(L)*2
　　
4.Calculate_Probability() :
　　主要工作： 由上一步得到的计数函数计算各个词项出现的概率（直接除以出现的总数即可），			同时进行Laplace_Smooth和log/exp映射（NaiveBayes2.py代码中还没进行）
　　输入：	词典L，word-y计数矩阵（Matrix_WordY）
　　输出：	word-y概率矩阵（同样是Matrix_WordY）

5.Prediction() :
　　主要工作：从测试集中(data2/TestSet)读取数据，预处理后(Filter过滤)，根据上面得到的				word-y概率矩阵，通过朴素贝叶斯算法来计算P0和P1的概率（连乘），并将			文档分到概率大的那个类别中。并且与flag进行对比(flag是文档的第一行，是			其应该被分到的类别，0/1)，每当flag与朴素贝叶斯算法得出的值不相等，				count_wrong便加一，最终用count_wrong/count_all，得到误分率。
　　输入：    两个上面统计出的概率矩阵矩阵，分别是Matrix_Y，Matrix_WordY，用来参与			朴素贝叶斯算法，预测文档的分类。
　　输出：    NULL，倒是会通过屏幕打印出误分率
　　
　　
Ps1:暂时未使用Laplace_Smooth和exp/log映射，当前的误分率是10%，使用后应该会更低――2015-8-7-16:50

Ps2:使用了Laplace_Smooth和exp/log映射，然而误分率还是10%，挺疑惑的，感觉是训练数据太少的缘故。2015-8-11

Ps3:去除停用词后可能误分率会下降。2015-8-11

附录：纸质版设计文档电子照片：


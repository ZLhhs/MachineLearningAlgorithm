LogisticRegression 详细设计手册

LiangZHANG
2015-8-13
Liangzxdu@foxmail.com

关键词：逻辑斯特回归、梯度下降、向量化、正则化

这是LogisticRegression.py代码的详细设计手册，该代码文件(LogisticRegression.py)和本文档(LogisticRegression.doc)同样位于LogisticRegression文件夹下。

LogisticRegression.py的数据来源于LogisticRegression文件夹下的data文件夹，其中包括训练集：train.txt和测试集：test.txt。
数据描述如下：
     每行数据都有22列，前21列是特征，即：x1,x2,x3,...,x21.最后一列代表y，即正样本或	负样本。


暂时没考虑正则化防止过拟合以及随机梯度下降算法（先实现最简单的再说，一步一步来）。
注意向量化，增强程序的可读性，同时也提高运行效率。

基本思路：
1. 读取训练数据和测试数据，得到Xrain, Ytrain, Xtest, Ytest （要加工下，方便后面使用）
2. LogisticRegression 初始化，设定theta。同时设置学习速率alpha，正则化开关/系数lambda
3. 梯度下降算法，即逻辑斯特回归学习过程，最优化theta。在使用的是批量梯度下降，没考虑  随机梯度下降和正则化防过拟合
4.在测试集上进行测试，计算出正确率。

主要函数剖析：
1.Read_Data :
　　主要工作：从数据文件中(LogisticRegression/data下的train.txt和test.txt)读取数据，构造训			练集和测试集.（记得在Xtrain和Xtest最左边加一列1）
　　输入：	NULL，也可以是文件路径地址
　　输出：	Xtrain, Ytrain, Xtest, Ytest

2.Initialization:
　　主要工作：初始化算法可能用到的各种参数（也可能直接在外面初始化），比如待学习的参			数theta（维度要和Xtrain 匹配，加一个1），学习速率alpha，迭代次数N，正			则化系数lambda等。
　　输入：	Xtrain
　　输出：    初始化的thate（初始化为全1）

3.Batch_Gradient_Descent () :
　　主要工作：运行批量梯度下降算法，优化参数theta。同时每次迭代后，根据当前theta计算			误差值J（误差函数），密切跟踪梯度下降算法，并以此来调节迭代次数N和学			习速率alpha，以及正则化系数lambda。
　　　　　　　 Ps:后来不仅计算了测试集的分类正确率(accuracy)，还计算了训练集的分类正			确率。同时计算出cost for train 和 cost for test.以及训练集的F值和测试集的F			值。
　　输入：	Xtrain，Ytrain，theta，lambda
　　输出：	训练好的theta，和一大堆记录数据(count_for_accuracy_F_and_Jcost_for_train_a
　　　　　　　 nd_test).
　　
4.Predication ():
　　主要工作：将梯度下降算法训练出的参数值theta带入测试集，计算预测结果，并与真实结			果进行比较，得出正确率。
　　输入：	参数theta，Xtest，Ytest
　　输出：	NULL，会打印出分类正确率
　　
5.Regularization():
　　主要工作：做正则化，防止logistic regression过拟合，同时尽可能加快其收敛。公式见设			计手册照片
　　输入：    lambda, m, theta，分别是正则化系数（控制正则化程度的，可以看成开关），数			据集数目，当前的参数
　　输出：    一个值，加到cost function上，用来实现正则化。
　　


Ps:后续会考虑随机梯度下降和正则化，当前版本还没有――正则化已经加了

Ps:使用matplot来画图，直观展示学习过程――画F值，Jcost，accuracy的变化图。

Ps:纸质版设计手册电子照片在本项目的photo文件夹中

Ps:做了regularization之后，准确率提高了，这个可以理解，但是cost function却也同时下降了，这个很奇怪，不是应该cost function增加么

Ps:学习速率alpha过大会导致学习曲线上下震荡！

Ps:加正则化貌似可以加快收敛速度。。。也能防止过拟合（不过本例中没有得到体现）
